from langchain.prompts import PromptTemplate
from langchain.chains import ConversationalRetrievalChain
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.memory import ConversationSummaryMemory
import sqlite3, os, time, json, uuid


# ==========================
# Cr√©ation de la base de donn√©es pour les analytics
# ==========================
def init_db():
    db_path = "data/analytics/analytics.db"
    os.makedirs(os.path.dirname(db_path), exist_ok=True)  # cr√©e le dossier si inexistant
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS chat_analytics (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        chat_id TEXT,
        intent TEXT,
        satisfaction_score REAL,
        chat_duration REAL,
        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
    )
    """)
    conn.commit()
    conn.close()

# ==========================
# Analyse LLM avec historique
# ==========================
def analyze_and_store_llm(user_message: str, agent_response: str, chat_history: str, duration: float):
    chat_id = str(uuid.uuid4())

    llm_analyse = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=0.2,
        api_key=os.getenv("GEMINI_API_KEY")
    )

    # Prompt prenant en compte l'historique complet
    analyse_prompt = PromptTemplate(
        input_variables=["chat_history", "user_message", "agent_response"],
        template="""
    Tu es un analyste conversationnel expert pour le support client Fnac.
    Analyse cette conversation compl√®te client-agent.

    Historique complet :
    {chat_history}

    Dernier message client :
    {user_message}

    Derni√®re r√©ponse agent :
    {agent_response}

    Pour chaque conversation, fournis un JSON avec :

    1Ô∏è‚É£ theme : le th√®me principal de la conversation (ex: commande, retour, produit, paiement, assistance technique)
    2Ô∏è‚É£ satisfaction_score : un nombre entre 0 et 1 repr√©sentant la satisfaction globale du client
    3Ô∏è‚É£ remarque : un court r√©sum√© des points importants ou frustrations √©ventuelles
    
    
    - Tous les guillemets doubles " dans le texte doivent √™tre √©chapp√©s avec un backslash \"
    - R√©pond strictement en JSON sous ce format :

    {{
    "theme": "retour",
    "satisfaction_score": 0.8,
    "remarque": "Le client voulait retourner un produit, agent a r√©pondu clairement"
    }}
    
    Ne pas utiliser de balises Markdown, renvoie uniquement le JSON pur.
    """
    )
    prompt_text = analyse_prompt.format(
        chat_history=chat_history,
        user_message=user_message,
        agent_response=agent_response
    )

    try:
        result = llm_analyse.invoke(prompt_text).content
        data = json.loads(result)
        intent = data.get("theme", "autre")
        satisfaction = float(data.get("satisfaction_score", 0.5))
        remarque = data.get("remarque", "")

    except Exception as e:
        intent = "autre"
        satisfaction = 0.5

    # Stockage minimal dans la base
    conn = sqlite3.connect("data/analytics/analytics.db")
    cursor = conn.cursor()
    cursor.execute("""
        INSERT INTO chat_analytics (chat_id, intent, satisfaction_score, chat_duration)
        VALUES (?, ?, ?, ?)
    """, (chat_id, intent, satisfaction, duration))
    conn.commit()
    conn.close()
    
        
def agent_support_fnac():
    embedding = HuggingFaceEmbeddings(model_name="embaas/sentence-transformers-multilingual-e5-base")
    vectordb = Chroma(persist_directory="./vectorstore/chroma", embedding_function=embedding)
    retriever = vectordb.as_retriever(search_kwargs={"k": 5})

    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash-lite", temperature=0.3, api_key=os.getenv("GEMINI_API_KEY"))
    
    llm_summary = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.2, api_key=os.getenv("GEMINI_API_KEY"))
    memory = ConversationSummaryMemory(llm=llm_summary, memory_key="chat_history", return_messages=True)

    # üîß Ton prompt personnalis√©
    template = """
    Tu es un agent de support client Fnac.

    - Ta mission : r√©pondre uniquement √† la QUESTION DU CLIENT en te basant uniquement sur le CONTEXTE fourni ci-dessous.
    - R√©ponds toujours poliment et de fa√ßon claire, avec les formules de courtoisie appropri√©es.
    - Si tu ne sais pas, dis-le explicitement ("Je ne dispose pas de cette information.").
    - Si la question sort du domaine Fnac, indique-le gentiment.
    - ‚ö†Ô∏è IMPORTANT : Si le client remercie, dit "ok", "d'accord", "merci", "au revoir" ou ferme la conversation, r√©ponds UNIQUEMENT par une courte phrase de politesse (ex: "De rien ! N'h√©sitez pas si vous avez d'autres questions."). NE R√âP√àTE JAMAIS ta r√©ponse pr√©c√©dente.
    - Ne redis pas "Bonjour" si tu l'as d√©j√† fait dans l'HISTORIQUE.
    - Ne r√©p√®te jamais exactement ce qui a d√©j√† √©t√© dit dans l'historique.


    === CONTEXTE ===
    {context}

    === HISTORIQUE ===
    {chat_history}

    === QUESTION DU CLIENT ===
    {question}

    === R√âPONSE ===
    """
    prompt = PromptTemplate(
        input_variables=["context", "chat_history", "question"],
        template=template,
    )

    # üîë CR√âER LA CHA√éNE UNE SEULE FOIS (pas √† chaque appel)
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        memory=memory,
        combine_docs_chain_kwargs={"prompt": prompt},
        output_key="answer",
        verbose=False
    )
    
    # Fonction appel√©e pour chaque message
    def run_support(query: str):
        response = qa_chain.invoke({"question": query})
        # Essaye plusieurs cl√©s possibles
        if isinstance(response, dict):
            answer = response.get("answer") or response.get("output_text") or str(response)
        else:
            answer = str(response)
        return answer

    return run_support, memory  # Retourne aussi la m√©moire pour l'analyse finale